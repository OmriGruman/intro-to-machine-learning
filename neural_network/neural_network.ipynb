{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement a neural network model. Our notebook will include the following stages:\n",
    "1. We will first load the MNIST data, and plot some of its image.\n",
    "2. Then, we will define some classes for some key part of the neural network architecture such as Weight Initializer, Activation, Loss Function and the Network model.\n",
    "3. Finally, we will conduct various experiments to come up with the optimal network architecture we can find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OmriG\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Basic\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "from functools import reduce\n",
    "from typing import Callable\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# DL\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 784), (60000,)), ((10000, 784), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten pixel images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "(X_train_flat.shape, y_train.shape), (X_test_flat.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x648 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot dimensions\n",
    "ROWS, COLS = 3, 5\n",
    "SCALE = 3\n",
    "\n",
    "# Sampled images\n",
    "pairs = list(zip(X_train, y_train))\n",
    "samples = random.sample(pairs, ROWS * COLS)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * SCALE, ROWS * SCALE))\n",
    "for i, (img, digit) in enumerate(samples):\n",
    "    row, col = i // COLS, i % COLS\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].set_title(digit)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sigmoid activation function as the non-linear layers of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def func(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, x):\n",
    "        sig = self.func(x)\n",
    "        return sig * (1 - sig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weight Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Xavier Normal initialization method that goes well with the Sigmoid activation function.\n",
    "\n",
    "https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier:\n",
    "    def init(self, layer_sizes):\n",
    "        return [\n",
    "            np.random.normal(loc=0, scale=np.sqrt(2 / (size_from + size_to)), size=(size_from, size_to))\n",
    "            for size_from, size_to in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is a multi-class classification task, and so we will define our loss function to be the Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "def one_hot(num_classes, labels):\n",
    "    return np.eye(num_classes)[labels].astype(bool)\n",
    "\n",
    "class CrossEntropyLoss:    \n",
    "    def func(self, y_true, y_pred):\n",
    "        return -np.log(softmax(y_pred)[one_hot(y_pred.shape[1], y_true)]).sum() / y_true.size\n",
    "    \n",
    "    def grad(self, y_true, y_pred):\n",
    "        return softmax(y_pred) - one_hot(y_pred.shape[1], y_true)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our Neural Network Classifier with some key components:\n",
    "1. Feed-forward\n",
    "2. Back-propagation\n",
    "3. Weights update\n",
    "4. Training loop\n",
    "5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_sizes, \n",
    "                 output_size, \n",
    "                 weight_init_type=Xavier, \n",
    "                 activation_type=Sigmoid,\n",
    "                 loss_type=CrossEntropyLoss,\n",
    "                 lr=1e-2,\n",
    "                 reg=0.0,\n",
    "                 max_iter=20, \n",
    "                 batch_size=100, \n",
    "                 validation_size=0.2, \n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "        self.weight_initializer = weight_init_type()\n",
    "        self.weights = self.weight_initializer.init(self.layer_sizes)\n",
    "        self.gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        self.activation = activation_type()\n",
    "        self.loss = loss_type()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.reg = reg        \n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_size = validation_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Inputs for each layer (hidden & output)\n",
    "        self.layer_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[:-1]]\n",
    "        self.activation_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[1:]]\n",
    "        \n",
    "        # First layer inputs\n",
    "        self.layer_inputs[0] = X\n",
    "        \n",
    "        # Calculate intermediate values\n",
    "        for layer, weight in enumerate(self.weights[:-1]):\n",
    "            \n",
    "            # Linear layer\n",
    "            self.activation_inputs[layer] = self.layer_inputs[layer] @ weight\n",
    "            \n",
    "            # Activation\n",
    "            self.layer_inputs[layer + 1] = self.activation.func(self.activation_inputs[layer])\n",
    "            \n",
    "        # Last linear layer\n",
    "        self.activation_inputs[-1] = self.layer_inputs[-1] @ self.weights[-1]\n",
    "        self.output = self.activation_inputs[-1]\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \n",
    "        # Gradient of loss w.r.t to last activation input\n",
    "        delta = self.loss.grad(y, self.activation_inputs[-1])\n",
    "        \n",
    "        # Iterate over layers\n",
    "        for layer in range(len(self.gradients) - 1, 0, -1):\n",
    "\n",
    "            # Gradient of loss w.r.t layer weights\n",
    "            self.gradients[layer] = self.layer_inputs[layer].T @ delta / y.size\n",
    "            \n",
    "            # Gradient of loss w.r.t to previous layer's input\n",
    "            delta = self.activation.grad(self.activation_inputs[layer - 1]) * (delta @ self.weights[layer].T)\n",
    "\n",
    "        # Last gradient of loss w.r.t first layer's weights\n",
    "        self.gradients[0] = self.layer_inputs[0].T @ delta / y.size\n",
    "\n",
    "    def step(self):\n",
    "        \n",
    "        # Update weights\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.weights[layer] -= self.lr * (grad + self.reg * self.weights[layer])\n",
    "\n",
    "    def single_iter(self, X, y, train=True):\n",
    "        num_batches = y.size / self.batch_size\n",
    "        iter_loss = 0\n",
    "        iter_num_correct = 0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for i in range(0, y.size, self.batch_size):\n",
    "            X_batch, y_batch = X[i:i+self.batch_size], y[i:i+self.batch_size]\n",
    "            self.forward(X_batch)\n",
    "        \n",
    "            # Back propagate\n",
    "            if train:\n",
    "                self.backward(y_batch)\n",
    "                self.step()\n",
    "                \n",
    "            # Calculate loss & accuracy\n",
    "            iter_loss += self.loss.func(y_batch, self.output)\n",
    "            iter_num_correct += (np.argmax(softmax(self.output), axis=1) == y_batch).sum()\n",
    "            \n",
    "        return iter_loss / num_batches, 100 * iter_num_correct / y.size\n",
    "        \n",
    "    def get_time_spent(self, total=False):\n",
    "        now = time.time()\n",
    "        base = self.step_time if not total else self.start_time\n",
    "        minutes = int(now - base) // 60 % 60\n",
    "        seconds = int(now - base) % 60\n",
    "        self.step_time = time.time()\n",
    "        \n",
    "        return f'{minutes:02d}:{seconds:02d}'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.start_time = time.time()\n",
    "        self.step_time = self.start_time\n",
    "        self.train_rows, self.val_rows = train_test_split(np.arange(X.shape[0]), test_size=self.validation_size, shuffle=self.shuffle, random_state=42)\n",
    "        print(self)\n",
    "        \n",
    "        # Training iterations\n",
    "        for self.iter in range(1, self.max_iter + 1):\n",
    "            print(f'----- Iteration {self.iter}/{self.max_iter} -----')\n",
    "            \n",
    "            train_loss, train_accuracy = self.single_iter(X[self.train_rows, :], y[self.train_rows])\n",
    "            print(f'  Train: Loss {train_loss:.3f}, Accuracy {train_accuracy:.2f} [{self.get_time_spent()}]')\n",
    "            \n",
    "            val_loss, val_accuracy = self.single_iter(X[self.val_rows, :], y[self.val_rows], train=False)\n",
    "            print(f'  Validation: Loss {val_loss:.3f}, Accuracy {val_accuracy:.2f} [{self.get_time_spent()}]')\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_accuracy)\n",
    "            \n",
    "        print(f'----- FINISHED [{self.get_time_spent(total=True)}] -----\\n')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return softmax(reduce(lambda x, w: self.activation.func(x @ w), [X, *self.weights[:-1]]) @ self.weights[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return (self.predict(X) == y).sum() / len(y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'[{\"-\".join(map(str, self.layer_sizes))}]_iter{self.max_iter}_lr{self.lr}_reg{self.reg}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define functions to help us save & load our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    with open(f'models/{model}', 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_model(model_path):\n",
    "    with open(f'models/{model_path}', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define functions to help us run experiments and print their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(nns):\n",
    "    num_iters = max([len(nn.train_losses) for nn in nns])\n",
    "    iters = np.arange(1, num_iters + 1)\n",
    "\n",
    "    # Plot Train & Validation Losses\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    for nn in nns:\n",
    "        ax[0][0].set_title(\"Train Loss\")\n",
    "        ax[0][0].plot(iters, nn.train_losses, label=nn)\n",
    "        ax[0][1].set_title(\"Validation Loss\")\n",
    "        ax[0][1].plot(iters, nn.val_losses, label=nn)\n",
    "        ax[1][0].set_title(\"Training Accuracy\")\n",
    "        ax[1][0].plot(iters, nn.train_accuracies, label=nn)\n",
    "        ax[1][1].set_title(\"Validation Accuracy\")\n",
    "        ax[1][1].plot(iters, nn.val_accuracies, label=nn)\n",
    "    \n",
    "    # Labels and Legends\n",
    "    for row in range(len(ax)):\n",
    "        for col in range(len(ax[0])):\n",
    "            ax[row][col].set_xlabel('Iterations')\n",
    "            ax[row][col].legend(loc='best')\n",
    "            \n",
    "    for col in range(len(ax[0])):\n",
    "        ax[0][col].set_ylabel('Loss')\n",
    "        ax[1][col].set_ylabel('Accuracy')\n",
    "    \n",
    "def plot_confusion_matrix(ax, clf, X, y, tag):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=cm, ax=ax)\n",
    "    ax.set_ylabel(\"Ground Truth\")\n",
    "    ax.set_xlabel(\"Predictions\")\n",
    "    ax.set_title(f'{clf}, {tag} score: {clf.score(X, y):.3f}')\n",
    "        \n",
    "def run_experiment(experiment, data_size):\n",
    "    for ex_args in experiment:\n",
    "        nn = NeuralNetworkClassifier(input_size=X_train_flat.shape[1], \n",
    "                                     output_size=np.unique(y_train).size,\n",
    "                                     **ex_args)\n",
    "        save_model(nn.fit(X_train_flat[:data_size], y_train[:data_size]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Network Width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1**: Different hidden layer sizes, single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_1 = [1024, 512, 256, 128]\n",
    "\n",
    "experiment_1 = [\n",
    "    {\n",
    "        \"hidden_sizes\": [size],\n",
    "    }\n",
    "    for size in layer_sizes_1\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_1, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size}-10]_iter20_lr0.01_reg0.0')\n",
    "    for size in layer_sizes_1\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Network Depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2**: Different hidden layer sizes, double layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_2 = [\n",
    "    [1024, 1024],\n",
    "    [1024, 512],\n",
    "    [1024, 256],\n",
    "    [512, 512]\n",
    "]\n",
    "\n",
    "experiment_2 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,   \n",
    "    }\n",
    "    for sizes in layer_sizes_2\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_2, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter20_lr0.01_reg0.0')\n",
    "    for size1, size2 in layer_sizes_2\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning Rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 3**: Different learning rates for single-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_3 = [1024, 512]\n",
    "lrs_3 = [1e-2, 3e-3, 1e-3, 3e-4]\n",
    "\n",
    "experiment_3 = [\n",
    "    {\n",
    "        \"hidden_sizes\": [size],\n",
    "        \"lr\": lr,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for lr in lrs_3\n",
    "    for size in layer_sizes_3\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_3, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_3\n",
    "    for size in layer_sizes_3\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4**: Different learning rates for double-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_4 = [[1024, 1024], [1024, 512]]\n",
    "lrs_4 = [3e-1, 1e-2, 3e-2, 1e-3]\n",
    "\n",
    "experiment_4 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,\n",
    "        \"lr\": lr,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for lr in lrs_4\n",
    "    for sizes in layer_sizes_4\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_4, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_4\n",
    "    for size1, size2 in layer_sizes_4\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Regulariztion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 5**: Different L2-regularization coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_6 = [[1024, 1024], [1024, 512]]\n",
    "regs = [1e-2, 3e-3, 1e-3, 3e-4]\n",
    "\n",
    "experiment_6 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,\n",
    "        \"lr\": 3e-2,\n",
    "        \"reg\": reg,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for reg in regs\n",
    "    for sizes in layer_sizes_6\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_6, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_4\n",
    "    for size1, size2 in layer_sizes_4\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
