{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from typing import Callable\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# DL\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten pixel images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "(X_train_flat.shape, y_train.shape), (X_test_flat.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimensions\n",
    "ROWS, COLS = 3, 5\n",
    "SCALE = 3\n",
    "\n",
    "# Sampled images\n",
    "pairs = list(zip(X_train, y_train))\n",
    "samples = random.sample(pairs, ROWS * COLS)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * SCALE, ROWS * SCALE))\n",
    "for i, (img, digit) in enumerate(samples):\n",
    "    row, col = i // COLS, i % COLS\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].set_title(digit)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsInitializer:\n",
    "    def init(self, layer_sizes):\n",
    "        assert False\n",
    "\n",
    "class Xavier(WeightsInitializer):\n",
    "    def init(self, layer_sizes):\n",
    "        return [\n",
    "            np.random.normal(loc=0, scale=np.sqrt(2 / (size_from + size_to)), size=(size_from, size_to))\n",
    "            for size_from, size_to in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "    \n",
    "class Kaiming(WeightsInitializer):\n",
    "    def init(self, layer_sizes):\n",
    "        return [\n",
    "            np.random.normal(loc=0, scale=np.sqrt(2 / size_from), size=(size_from, size_to))\n",
    "            for size_from, size_to in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def func(self, x):\n",
    "        assert False\n",
    "        \n",
    "    def grad(self, x):\n",
    "        assert False\n",
    "        \n",
    "class ReLU(Activation):\n",
    "    def func(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "class LeakyReLU(Activation):\n",
    "    def func(self, x):\n",
    "        return np.maximum(x, 0.01 * x)\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def func(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, x):\n",
    "        sig = self.func(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "class Tanh(Activation):\n",
    "    def func(self, x):\n",
    "        exp = np.exp(x)\n",
    "        return (exp + (1 / exp)) / (exp - (1 / exp))\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return 1 - self.func(x) ** 2\n",
    "    \n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, probabilities):\n",
    "    return -np.log(probabilities[np.arange(y_true.size), y_true]).sum() / y_true.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, weights, gradients, lr, weight_decay):\n",
    "        self.weights = weights\n",
    "        self.gradients = gradients\n",
    "        self.lr = lr\n",
    "        self.weight_decay\n",
    "        \n",
    "    def _get_gradients(self):\n",
    "        assert False\n",
    "        \n",
    "    def step(self):\n",
    "        for layer, grad in enumerate(self._get_gradients()):\n",
    "            self.weights[layer] -= self.lr * grad\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr):\n",
    "        super().__init__(weights, gradients, lr)\n",
    "        \n",
    "    def _get_gradients(self):\n",
    "        return self.gradients\n",
    "\n",
    "class SGDMomentom(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr, beta=0.9):\n",
    "        super().__init__(weights, gradients, lr)\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.vt = [np.zeros_like(w) for w in self.weights]\n",
    "    \n",
    "    def _get_gradients(self):\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.vt[layer] = self.beta * self.vt[layer] + (1 - self.beta) * (grad + self.weights * self.weight_decay)\n",
    "            \n",
    "        return self.vt\n",
    "\n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr):\n",
    "        super().__init__(weights, gradients, lr)\n",
    "        \n",
    "        self.grad_squared = [np.zeros_like(w) for w in self.weights]\n",
    "        self.vt = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "    def _get_gradients(self):\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.grad_squared[layer] += (grad + self.weights * self.weight_decay) ** 2\n",
    "            self.vt[layer] = 1 / np.sqrt(self.grad_squared[layer] + 1e-8)\n",
    "            \n",
    "        return self.vt\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr, beta=0.9):\n",
    "        super().__init__(weights, gradients, lr)\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.grad_squared = [np.zeros_like(w) for w in self.weights]\n",
    "        self.vt = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "    def _get_gradients(self):\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.grad_squared[layer] += self.beta * self.grad_squared[layer] + (1 - beta) * (grad + self.weights * self.weight_decay) ** 2\n",
    "            self.vt[layer] = 1 / np.sqrt(self.eta[layer] + 1e-8)\n",
    "            \n",
    "        return self.vt\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr, beta1=0.9, beta2=0.999):\n",
    "        super().__init__(weights, gradients)\n",
    "        \n",
    "        self.t = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.mt = [np.zeros_like(w) for w in self.weights]\n",
    "        self.vt = [np.zeros_like(w) for w in self.weights]\n",
    "        self.total = [np.zeros_like(w) for w in self.weights]\n",
    "          \n",
    "    def _get_gradients(self):\n",
    "        self.t += 1\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.mt[layer] = (self.beta1 * self.mt[layer] + (1 - self.beta1) * (grad + self.weights * self.weight_decay)) / (1 - self.beta1 ** self.t)\n",
    "            self.vt[layer] = (self.beta2 * self.vt[layer] + (1 - self.beta2) * ((grad + self.weights * self.weight_decay) ** 2)) / (1 - self.beta2 ** self.t)\n",
    "            self.total[layer] = self.mt[layer] / (np.sqrt(self.vt[layer]) + 1e-8)\n",
    "            \n",
    "        return self.total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_sizes, \n",
    "                 output_size, \n",
    "                 weight_init_type, \n",
    "                 activation_type, \n",
    "                 optimizer_type, \n",
    "                 lr,\n",
    "                 weight_decay,\n",
    "                 max_iter, \n",
    "                 batch_size=100, \n",
    "                 validation_size=0.2, \n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "        self.weights = weight_init_type().init(self.layer_sizes)\n",
    "        self.gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        self.activation = activation_type()\n",
    "        self.optimizer = optimizer_type(self.weights, self.gradients, lr)\n",
    "        \n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_size = validation_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Inputs for each layer (hidden & output)\n",
    "        self.layer_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[:-1]]\n",
    "        self.activation_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[1:]]\n",
    "        \n",
    "        # First layer inputs\n",
    "        self.layer_inputs[0] = X\n",
    "        \n",
    "        # Calculate intermediate values\n",
    "        for layer, weight in enumerate(self.weights[:-1]):\n",
    "            \n",
    "            # Linear layer\n",
    "            self.activation_inputs[layer] = self.layer_inputs[layer] @ weight\n",
    "            \n",
    "            # Activation\n",
    "            self.layer_inputs[layer + 1] = self.activation.func(self.activation_inputs[layer])\n",
    "            \n",
    "        # Last linear layer\n",
    "        self.activation_inputs[-1] = self.layer_inputs[-1] @ self.weights[-1]\n",
    "        \n",
    "        # Logits\n",
    "        self.output = softmax(self.activation_inputs[-1])\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \n",
    "        # Gradient of loss w.r.t to softmax input\n",
    "        one_hot_labels = np.zeros_like(self.output)\n",
    "        one_hot_labels[np.arange(y.size), y] = 1\n",
    "        delta = self.output - one_hot_labels\n",
    "        \n",
    "        # Iterate over layers\n",
    "        for layer in range(len(self.gradients) - 1, 0, -1):\n",
    "            \n",
    "            # Gradient of loss w.r.t layer weights\n",
    "            self.gradients[layer] = self.layer_inputs[layer].T @ delta / y.size\n",
    "            \n",
    "            # Gradient of loss w.r.t to previous layer's input\n",
    "            delta = self.activation.grad(self.activation_inputs[layer - 1]) * (delta @ self.weights[layer].T)\n",
    "\n",
    "        # Last gradient of loss w.r.t first layer's weights\n",
    "        self.gradients[0] = self.layer_inputs[0].T @ delta / y.size\n",
    "\n",
    "    def single_iter(self, X, y, train=True):\n",
    "        num_batches = y.size / self.batch_size\n",
    "        iter_loss = 0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for i in range(0, y.size, self.batch_size):\n",
    "            X_batch, y_batch = X[i:i+self.batch_size], y[i:i+self.batch_size]\n",
    "        \n",
    "            # Validate\n",
    "            if not train:\n",
    "                self.output = self.predict_proba(X_batch)\n",
    "                \n",
    "            # Train\n",
    "            else:\n",
    "                self.forward(X_batch)\n",
    "                self.backward(y_batch)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            iter_loss += cross_entropy_loss(y_batch, self.output)\n",
    "            \n",
    "        return iter_loss / num_batches\n",
    "        \n",
    "    def print_status(self):\n",
    "        now = time.time()\n",
    "        hours = int(now - self.start_time) // 3600\n",
    "        minutes = int(now - self.start_time) // 60 % 60\n",
    "        seconds = int(now - self.start_time) % 60\n",
    "        percent = int(100 * self.iter / self.max_iter)\n",
    "        \n",
    "        status = f'Train Loss ({self.train_losses[-1]:.3f}), Validation Loss ({self.val_losses[-1]:.3f}): {percent}% {self.iter}/{self.max_iter} [{hours:02d}:{minutes:02d}:{seconds:02d}]'\n",
    "        print(status, end='\\r' if self.iter < self.max_iter else '\\n')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.start_time = time.time()\n",
    "        self.train_rows, self.val_rows = train_test_split(np.arange(X.shape[0]), test_size=self.validation_size, shuffle=self.shuffle, random_state=42)\n",
    "        print(self)\n",
    "        \n",
    "        # Training iterations\n",
    "        for self.iter in range(1, self.max_iter + 1):\n",
    "            self.train_losses.append(self.single_iter(X[self.train_rows, :], y[self.train_rows]))\n",
    "            self.val_losses.append(self.single_iter(X[self.val_rows, :], y[self.val_rows], train=False))\n",
    "            self.print_status()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return softmax(reduce(lambda x, w: self.activation.func(x @ w), [X, *self.weights[:-1]]) @ self.weights[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return (self.predict(X) == y).sum() / len(y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'[{\"-\".join(map(str, self.layer_sizes))}]_{type(self.activation).__name__}_{type(self.optimizer).__name__}_lr{self.optimizer.lr}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(ax, nn):\n",
    "    num_iters = len(nn.train_losses)\n",
    "    iters = np.arange(1, num_iters + 1)\n",
    "\n",
    "    # Plot Train & Validation Losses\n",
    "    ax.plot(iters, nn.train_losses, label='Training Loss')\n",
    "    ax.plot(iters, nn.val_losses, label='Validation Loss')\n",
    "    ax.set_title(nn)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "def plot_confusion_matrix(ax, clf, X, y, tag):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=cm, ax=ax)\n",
    "    ax.set_ylabel(\"Ground Truth\")\n",
    "    ax.set_xlabel(\"Predictions\")\n",
    "    ax.set_title(f'{clf}, {tag} score: {clf.score(X, y):.3f}')\n",
    "\n",
    "def plot_experiment_summary(nns):\n",
    "    \n",
    "    # Plot learning curves\n",
    "    fig, ax = plt.subplots(1, len(nns), figsize=(10 * len(nns), 10), sharey=True)\n",
    "    for i, nn in enumerate(nns):\n",
    "        plot_learning_curves(ax[i], nn)\n",
    "        \n",
    "    # Plot confusion matrices\n",
    "    fig, ax = plt.subplots(2, len(nns), figsize=(10 * len(nns), 20))\n",
    "    for i, nn in enumerate(nns):\n",
    "        plot_confusion_matrix(ax[0, i], nn, X_train_flat[nn.train_rows, :], y_train[nn.train_rows], \"Train\")\n",
    "        plot_confusion_matrix(ax[1, i], nn, X_train_flat[nn.val_rows, :], y_train[nn.val_rows], \"Validation\")\n",
    "        \n",
    "def run_experiment(experiment, data_size):\n",
    "    nns = []\n",
    "    for ex_args in experiment:\n",
    "        nn = NeuralNetwork(input_size=X_train_flat.shape[1], \n",
    "                           output_size=10,\n",
    "                           **ex_args)\n",
    "        \n",
    "        nns.append(nn.fit(X_train_flat[:data_size], y_train[:data_size]))\n",
    "    \n",
    "    plot_experiment_summary(nns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hidden Layer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1 = [\n",
    "    {\n",
    "        \"hidden_sizes\": [1024],\n",
    "        \"weight_init_type\": Xavier, \n",
    "        \"activation_type\": Sigmoid,\n",
    "        \"optimizer_type\": SGD,\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 0,\n",
    "        \"max_iter\": 10\n",
    "    },\n",
    "    {\n",
    "        \"hidden_sizes\": [512],\n",
    "        \"weight_init_type\": Xavier, \n",
    "        \"activation_type\": Sigmoid,\n",
    "        \"optimizer_type\": SGD,\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 0,\n",
    "        \"max_iter\": 10\n",
    "    },\n",
    "    {\n",
    "        \"hidden_sizes\": [256],\n",
    "        \"weight_init_type\": Xavier, \n",
    "        \"activation_type\": Sigmoid,\n",
    "        \"optimizer_type\": SGD,\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 0,\n",
    "        \"max_iter\": 10\n",
    "    },\n",
    "    {\n",
    "        \"hidden_sizes\": [128],\n",
    "        \"weight_init_type\": Xavier, \n",
    "        \"activation_type\": Sigmoid,\n",
    "        \"optimizer_type\": SGD,\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 0,\n",
    "        \"max_iter\": 10\n",
    "    },\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_1, data_size=X_train.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimizer & Learning Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Regulariztion & Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Activations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Network Depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 More Iteration & Learning Rate Scheduling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
