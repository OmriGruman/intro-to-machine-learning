{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement a neural network model. Our notebook will include the following stages:\n",
    "1. We will first load the MNIST data, and plot some of its image.\n",
    "2. Then, we will define some classes for some key part of the neural network architecture such as Weight Initializer, Activation, Loss Function and the Network model.\n",
    "3. Finally, we will conduct various experiments to come up with the optimal network architecture we can find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "from functools import reduce\n",
    "from typing import Callable\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    X = np.load('MNIST-data.npy')\n",
    "    y = np.load(\"MNIST-lables.npy\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, shuffle=True, random_state=42)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_mnist()\n",
    "\n",
    "# Flatten pixel images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "(X_train_flat.shape, y_train.shape), (X_test_flat.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimensions\n",
    "ROWS, COLS = 3, 5\n",
    "SCALE = 3\n",
    "\n",
    "# Sampled images\n",
    "pairs = list(zip(X_train, y_train))\n",
    "samples = random.sample(pairs, ROWS * COLS)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * SCALE, ROWS * SCALE))\n",
    "for i, (img, digit) in enumerate(samples):\n",
    "    row, col = i // COLS, i % COLS\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].set_title(digit)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sigmoid activation function as the non-linear layers of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def func(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, x):\n",
    "        sig = self.func(x)\n",
    "        return sig * (1 - sig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weight Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Xavier Normal initialization method that goes well with the Sigmoid activation function.\n",
    "\n",
    "https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier:\n",
    "    def init(self, layer_sizes):\n",
    "        return [\n",
    "            np.random.normal(loc=0, scale=np.sqrt(2 / (size_from + size_to)), size=(size_from, size_to))\n",
    "            for size_from, size_to in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is a multi-class classification task, and so we will define our loss function to be the Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "def one_hot(num_classes, labels):\n",
    "    return np.eye(num_classes)[labels].astype(bool)\n",
    "\n",
    "class CrossEntropyLoss:    \n",
    "    def func(self, y_true, y_pred):\n",
    "        return -np.log(softmax(y_pred)[one_hot(y_pred.shape[1], y_true)]).sum() / y_true.size\n",
    "    \n",
    "    def grad(self, y_true, y_pred):\n",
    "        return softmax(y_pred) - one_hot(y_pred.shape[1], y_true)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our Neural Network Classifier with some key components:\n",
    "1. Feed-forward\n",
    "2. Back-propagation\n",
    "3. Weights update\n",
    "4. Training loop\n",
    "5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_sizes, \n",
    "                 output_size, \n",
    "                 weight_init_type=Xavier, \n",
    "                 activation_type=Sigmoid,\n",
    "                 loss_type=CrossEntropyLoss,\n",
    "                 lr=1e-2,\n",
    "                 reg=0.0,\n",
    "                 max_iter=20, \n",
    "                 batch_size=100, \n",
    "                 validation_size=0.2, \n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "        self.weight_initializer = weight_init_type()\n",
    "        self.weights = self.weight_initializer.init(self.layer_sizes)\n",
    "        self.gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        self.activation = activation_type()\n",
    "        self.loss = loss_type()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.reg = reg        \n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_size = validation_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Inputs for each layer (hidden & output)\n",
    "        self.layer_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[:-1]]\n",
    "        self.activation_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[1:]]\n",
    "        \n",
    "        # First layer inputs\n",
    "        self.layer_inputs[0] = X\n",
    "        \n",
    "        # Calculate intermediate values\n",
    "        for layer, weight in enumerate(self.weights[:-1]):\n",
    "            \n",
    "            # Linear layer\n",
    "            self.activation_inputs[layer] = self.layer_inputs[layer] @ weight\n",
    "            \n",
    "            # Activation\n",
    "            self.layer_inputs[layer + 1] = self.activation.func(self.activation_inputs[layer])\n",
    "            \n",
    "        # Last linear layer\n",
    "        self.activation_inputs[-1] = self.layer_inputs[-1] @ self.weights[-1]\n",
    "        self.output = self.activation_inputs[-1]\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \n",
    "        # Gradient of loss w.r.t to last activation input\n",
    "        delta = self.loss.grad(y, self.activation_inputs[-1])\n",
    "        \n",
    "        # Iterate over layers\n",
    "        for layer in range(len(self.gradients) - 1, 0, -1):\n",
    "\n",
    "            # Gradient of loss w.r.t layer weights\n",
    "            self.gradients[layer] = self.layer_inputs[layer].T @ delta / y.size\n",
    "            \n",
    "            # Gradient of loss w.r.t to previous layer's input\n",
    "            delta = self.activation.grad(self.activation_inputs[layer - 1]) * (delta @ self.weights[layer].T)\n",
    "\n",
    "        # Last gradient of loss w.r.t first layer's weights\n",
    "        self.gradients[0] = self.layer_inputs[0].T @ delta / y.size\n",
    "\n",
    "    def step(self):\n",
    "        \n",
    "        # Update weights\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            self.weights[layer] -= self.lr * (grad + self.reg * self.weights[layer])\n",
    "\n",
    "    def single_iter(self, X, y, train=True):\n",
    "        num_batches = y.size / self.batch_size\n",
    "        iter_loss = 0\n",
    "        iter_num_correct = 0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for i in range(0, y.size, self.batch_size):\n",
    "            X_batch, y_batch = X[i:i+self.batch_size], y[i:i+self.batch_size]\n",
    "            self.forward(X_batch)\n",
    "        \n",
    "            # Back propagate\n",
    "            if train:\n",
    "                self.backward(y_batch)\n",
    "                self.step()\n",
    "                \n",
    "            # Calculate loss & accuracy\n",
    "            iter_loss += self.loss.func(y_batch, self.output)\n",
    "            iter_num_correct += (np.argmax(softmax(self.output), axis=1) == y_batch).sum()\n",
    "            \n",
    "        return iter_loss / num_batches, 100 * iter_num_correct / y.size\n",
    "        \n",
    "    def get_time_spent(self, total=False):\n",
    "        now = time.time()\n",
    "        base = self.step_time if not total else self.start_time\n",
    "        minutes = int(now - base) // 60 % 60\n",
    "        seconds = int(now - base) % 60\n",
    "        self.step_time = time.time()\n",
    "        \n",
    "        return f'{minutes:02d}:{seconds:02d}'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.start_time = time.time()\n",
    "        self.step_time = self.start_time\n",
    "        self.train_rows, self.val_rows = train_test_split(np.arange(X.shape[0]), test_size=self.validation_size, shuffle=self.shuffle, random_state=42)\n",
    "        print(self)\n",
    "        \n",
    "        # Training iterations\n",
    "        for self.iter in range(1, self.max_iter + 1):\n",
    "            print(f'----- Iteration {self.iter}/{self.max_iter} -----')\n",
    "            \n",
    "            train_loss, train_accuracy = self.single_iter(X[self.train_rows, :], y[self.train_rows])\n",
    "            print(f'  Train: Loss {train_loss:.3f}, Accuracy {train_accuracy:.2f} [{self.get_time_spent()}]')\n",
    "            \n",
    "            val_loss, val_accuracy = self.single_iter(X[self.val_rows, :], y[self.val_rows], train=False)\n",
    "            print(f'  Validation: Loss {val_loss:.3f}, Accuracy {val_accuracy:.2f} [{self.get_time_spent()}]')\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_accuracy)\n",
    "            \n",
    "        print(f'----- FINISHED [{self.get_time_spent(total=True)}] -----\\n')\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return softmax(reduce(lambda x, w: self.activation.func(x @ w), [X, *self.weights[:-1]]) @ self.weights[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return (self.predict(X) == y).sum() / len(y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'[{\"-\".join(map(str, self.layer_sizes))}]_iter{self.max_iter}_lr{self.lr}_reg{self.reg}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define functions to help us save & load our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    with open(f'models/{model}', 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_model(model_path):\n",
    "    with open(f'models/{model_path}', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define functions to help us run experiments and print their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(nns):\n",
    "    num_iters = max([len(nn.train_losses) for nn in nns])\n",
    "    iters = np.arange(1, num_iters + 1)\n",
    "\n",
    "    # Plot Train & Validation Losses\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    for nn in nns:\n",
    "        ax[0][0].set_title(\"Train Loss\")\n",
    "        ax[0][0].plot(iters, nn.train_losses, label=nn)\n",
    "        ax[0][1].set_title(\"Validation Loss\")\n",
    "        ax[0][1].plot(iters, nn.val_losses, label=nn)\n",
    "        ax[1][0].set_title(\"Training Accuracy\")\n",
    "        ax[1][0].plot(iters, nn.train_accuracies, label=nn)\n",
    "        ax[1][1].set_title(\"Validation Accuracy\")\n",
    "        ax[1][1].plot(iters, nn.val_accuracies, label=nn)\n",
    "    \n",
    "    # Labels and Legends\n",
    "    for row in range(len(ax)):\n",
    "        for col in range(len(ax[0])):\n",
    "            ax[row][col].set_xlabel('Iterations')\n",
    "            ax[row][col].legend(loc='best')\n",
    "            \n",
    "    for col in range(len(ax[0])):\n",
    "        ax[0][col].set_ylabel('Loss')\n",
    "        ax[1][col].set_ylabel('Accuracy')\n",
    "    \n",
    "def plot_confusion_matrix(ax, clf, X, y, tag):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=cm, ax=ax)\n",
    "    ax.set_ylabel(\"Ground Truth\")\n",
    "    ax.set_xlabel(\"Predictions\")\n",
    "    ax.set_title(f'{clf}, {tag} score: {clf.score(X, y):.3f}')\n",
    "        \n",
    "def run_experiment(experiment, data_size):\n",
    "    for ex_args in experiment:\n",
    "        nn = NeuralNetworkClassifier(input_size=X_train_flat.shape[1], \n",
    "                                     output_size=np.unique(y_train).size,\n",
    "                                     **ex_args)\n",
    "        save_model(nn.fit(X_train_flat[:data_size], y_train[:data_size]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Network Width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1**: Different hidden layer sizes, single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_1 = [1024, 512, 256, 128]\n",
    "\n",
    "experiment_1 = [\n",
    "    {\n",
    "        \"hidden_sizes\": [size],\n",
    "    }\n",
    "    for size in layer_sizes_1\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_1, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size}-10]_iter20_lr0.01_reg0.0')\n",
    "    for size in layer_sizes_1\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Network Depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2**: Different hidden layer sizes, double layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_2 = [\n",
    "    [1024, 1024],\n",
    "    [1024, 512],\n",
    "    [1024, 256],\n",
    "    [512, 512]\n",
    "]\n",
    "\n",
    "experiment_2 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,   \n",
    "    }\n",
    "    for sizes in layer_sizes_2\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_2, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter20_lr0.01_reg0.0')\n",
    "    for size1, size2 in layer_sizes_2\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning Rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 3**: Different learning rates for single-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_3 = [1024, 512]\n",
    "lrs_3 = [1e-2, 3e-3, 1e-3, 3e-4]\n",
    "\n",
    "experiment_3 = [\n",
    "    {\n",
    "        \"hidden_sizes\": [size],\n",
    "        \"lr\": lr,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for lr in lrs_3\n",
    "    for size in layer_sizes_3\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_3, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_3\n",
    "    for size in layer_sizes_3\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4**: Different learning rates for double-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_4 = [[1024, 1024], [1024, 512]]\n",
    "lrs_4 = [3e-1, 1e-2, 3e-2, 1e-3]\n",
    "\n",
    "experiment_4 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,\n",
    "        \"lr\": lr,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for lr in lrs_4\n",
    "    for sizes in layer_sizes_4\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_4, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_4\n",
    "    for size1, size2 in layer_sizes_4\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Regulariztion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 5**: Different L2-regularization coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_6 = [[1024, 1024], [1024, 512]]\n",
    "regs = [1e-2, 3e-3, 1e-3, 3e-4]\n",
    "\n",
    "experiment_6 = [\n",
    "    {\n",
    "        \"hidden_sizes\": sizes,\n",
    "        \"lr\": 3e-2,\n",
    "        \"reg\": reg,\n",
    "        \"max_iter\": 40\n",
    "    }\n",
    "    for reg in regs\n",
    "    for sizes in layer_sizes_6\n",
    "]\n",
    "\n",
    "run_experiment(experiment=experiment_6, data_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([\n",
    "    load_model(f'[784-{size1}-{size2}-10]_iter40_lr{lr}_reg0.0')\n",
    "    for lr in lrs_4\n",
    "    for size1, size2 in layer_sizes_4\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
