{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# DL\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=y_test.size, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_val = X_val / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimensions\n",
    "ROWS, COLS = 3, 5\n",
    "SCALE = 3\n",
    "\n",
    "# Sampled images\n",
    "pairs = list(zip(X_train, y_train))\n",
    "samples = random.sample(pairs, ROWS * COLS)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS * SCALE, ROWS * SCALE))\n",
    "for i, (img, digit) in enumerate(samples):\n",
    "    row, col = i // COLS, i % COLS\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].set_title(digit)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def func(self, x):\n",
    "        assert False\n",
    "        \n",
    "    def grad(self, x):\n",
    "        assert False\n",
    "        \n",
    "class ReLU(Activation):\n",
    "    def func(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def func(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def grad(self, x):\n",
    "        sig = sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "class Tanh(Activation):\n",
    "    def func(self, x):\n",
    "        exp = np.exp(x)\n",
    "        return (exp + (1 / exp)) / (exp - (1 / exp))\n",
    "    \n",
    "    def grad(self, x):\n",
    "        return 1 - self.func(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, weights, gradients):\n",
    "        self.weights = weights\n",
    "        self.gradients = gradients\n",
    "        \n",
    "    def step(gradients):\n",
    "        assert False\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, weights, gradients, lr=3e-4, beta1=0.9, beta2=0.999):\n",
    "        super().__init__(weights, gradients)\n",
    "        \n",
    "        self.t = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.mt = 0\n",
    "        self.vt = 0\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        # Update weights\n",
    "        for layer, grad in enumerate(self.gradients):\n",
    "            \n",
    "            new_mt = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            new_vt = self.beta2 * self.vt + (1 - self.beta2) * (grad**2)\n",
    "            \n",
    "            new_mt = new_mt / (1 - self.beta1 ** self.t)\n",
    "            new_vt = new_vt / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            self.weights[layer] -= self.lr * new_mt / (np.sqrt(new_vt) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_type, optimizer_type, lr=3e-4, max_iter=1000, verbose=True):\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.layer_sizes = [input_size, *hidden_sizes, output_size]\n",
    "        self.weights = [\n",
    "            np.random.uniform(0, 1, size=(size_from, size_to)) / (size_from / 2)\n",
    "            for size_from, size_to in zip(self.layer_sizes[:-1], self.layer_sizes[1:])\n",
    "        ]\n",
    "        self.gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "        self.activation = activation_type()\n",
    "        self.optimizer = optimizer_type(self.weights, self.gradients, lr)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Inputs for each layer (hidden & output)\n",
    "        self.layer_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[:-1]]\n",
    "        self.activation_inputs = [np.zeros((X.shape[0], size)) for size in self.layer_sizes[1:]]\n",
    "        \n",
    "        # First layer inputs\n",
    "        self.layer_inputs[0] = X\n",
    "        \n",
    "        # Calculate intermediate values\n",
    "        for layer, weight in enumerate(self.weights[:-1]):\n",
    "            \n",
    "            # Linear layer\n",
    "            self.activation_inputs[layer] = self.layer_inputs[layer] @ weight\n",
    "            \n",
    "            # Activation\n",
    "            self.layer_inputs[layer + 1] = self.activation.func(self.activation_inputs[layer])\n",
    "            \n",
    "        # Last linear layer\n",
    "        self.activation_inputs[-1] = self.layer_inputs[-1] @ self.weights[-1]\n",
    "        \n",
    "        # Logits\n",
    "        self.output = softmax(self.activation_inputs[-1])\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \n",
    "        # Zero gradients\n",
    "        for grad in self.gradients:\n",
    "            grad.fill(0)\n",
    "        \n",
    "        # Gradient of loss w.r.t to softmax input\n",
    "        one_hot_labels = np.zeros_like(self.output)\n",
    "        one_hot_labels[np.arange(y.size), y] = 1\n",
    "        delta = self.output - one_hot_labels\n",
    "        \n",
    "        # Iterate over layers\n",
    "        for layer in range(len(self.gradients) - 1, 0, -1):\n",
    "            \n",
    "            # Gradient of loss w.r.t layer weights\n",
    "            self.gradients[layer] = self.layer_inputs[layer].T @ delta\n",
    "            \n",
    "            # Gradient of loss w.r.t to previous layer's input\n",
    "            delta = self.activation.grad(self.activation_inputs[layer - 1]) * (delta @ self.weights[layer].T)\n",
    "\n",
    "        # Last gradient of loss w.r.t first layer's weights\n",
    "        self.gradients[0] = self.layer_inputs[0].T @ delta\n",
    "        \n",
    "        # Cross-Entropy loss\n",
    "        self.loss = -np.log(softmax(self.activation_inputs[-1])[one_hot_labels.astype(bool)]).sum() / y.size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Iterating over data        \n",
    "        for itr in range(1, self.max_iter + 1):\n",
    "            \n",
    "            self.forward(X)\n",
    "            self.backward(y)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if itr % 100 == 0:\n",
    "                print(f'iteration {itr}/{self.max_iter}')\n",
    "                print(f'    loss: {self.loss}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return softmax(reduce(lambda x, w: self.activation.func(x @ w), [X, *self.weights[:-1]]) @ self.weights[-1])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        return (self.predict(X) == y).sum() / len(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf = NeuralNetwork(input_size=X_train_flat.shape[1], \n",
    "                       hidden_sizes=[256, 256],\n",
    "                       output_size=np.unique(y_train).size,\n",
    "                       activation_type=ReLU,\n",
    "                       optimizer_type=Adam)\n",
    "\n",
    "nn_clf.fit(X_train_flat, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(clf, X, y, tag):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.heatmap(cm, annot=cm)\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.title(f'{tag}: {clf.layer_sizes}, Score: {100 * clf.score(X, y):.2f}%')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(nn_clf, X_train_flat, y_train, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(nn_clf, X_val_flat, y_val, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(nn_clf, X_test_flat, y_test, \"Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
